from datapipelines.generate_vendas.commons.spark_session import SparkSessionWrapper
from datapipelines.generate_vendas.commons.etl_steps import (
    load_raw_data,
    apply_transformations,
    save_processed_data
)
from datapipelines.logger_config import get_logger

logger = get_logger("raw_to_processed_vendas")


def main():
    logger.info("â–¶ Iniciando sessÃ£o Spark para carga da camada processed.")
    spark_wrapper = SparkSessionWrapper(app_name="GenerateVendasSparkSession")
    spark = spark_wrapper.get_session()

    try:
        logger.info("ğŸ“¥ Lendo dados brutos da camada raw...")
        df_raws = load_raw_data(spark)

        logger.info("ğŸ”§ Aplicando transformaÃ§Ãµes...")
        df_processed = apply_transformations(df_raws)

        logger.info("ğŸ’¾ Salvando arquivos na camada processed...")
        save_processed_data(df_processed)

        logger.info("âœ… Pipeline raw_to_processed finalizada com sucesso.")
    except Exception as e:
        logger.exception("âŒ Erro na pipeline raw_to_processed:")
        raise
    finally:
        spark_wrapper.stop()
        logger.info("ğŸ›‘ SessÃ£o Spark finalizada.")


if __name__ == "__main__":
    main()
