# ETL Pipeline AWS Real Case 

## ğŸ¯ Objetivo do Projeto

ConstruÃ§Ã£o de pipelines **ETL no Amazon EC2** utilizando **Apache Spark** e orquestradas pelo **Apache Airflow**, para processar e transformar dados brutos das Ã¡reas de **Clientes** e **Vendas**, gerando camadas intermediÃ¡rias (`processed`) e finais (`gold`) diretamente no **Amazon S3**, seguindo boas prÃ¡ticas de engenharia de dados, modularidade de cÃ³digo e padrÃµes corporativos.

## ğŸ§© Arquitetura do Projeto

![Arquitetura do Projeto](arquitetura.png)

### ğŸ§ª **Ambiente de ExecuÃ§Ã£o**

O ambiente foi construÃ­do utilizando contÃªineres Docker orquestrados via `docker-compose`, com **Apache Airflow** como orquestrador de tarefas e **Apache Spark** como motor de processamento distribuÃ­do.

---

### âœ… 1. PrÃ©-requisitos

- Conta ativa na **AWS**
- Chave de acesso (`.pem`) para conexÃ£o via SSH
- **AWS CLI** configurada localmente (`aws configure`)

## ğŸš€ 2. Clonar o RepositÃ³rio

```bash
git clone https://github.com/MatheusNBrito/etl-pipeline-aws-real-case.git
cd etl-pipeline-aws-real-case

## âš™ï¸ 3. Ajustes NecessÃ¡rios

Editar apenas o essencial:

- Arquivo: `IaC/terraform.tfvars`

    Ajustar o valor da chave SSH:

    ```hcl
    key_name = "nome-da-sua-chave-aws.pem"
    ```

- Demais variÃ¡veis (`bucket_name`, `instance_type`, etc.) sÃ³ alterar se necessÃ¡rio.

## â˜ï¸ 4. Provisionar a Infraestrutura com Terraform (via container)

Dentro da pasta `IaC/`:

### ğŸ”¹ Inicializar o Terraform:

```bash
docker run -it --rm \
  -v $(pwd):/app \
  -w /app \
  hashicorp/terraform:light init

### ğŸ”¹ Aplicar o Terraform:

```bash
docker run -it --rm \
  -v $(pwd):/app \
  -w /app \
  -e AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id) \
  -e AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) \
  hashicorp/terraform:light apply -var-file=terraform.tfvars

## ğŸ” 5. Conectar na InstÃ¢ncia EC2

Antes de conectar, ajuste as permissÃµes da chave:

```bash
chmod 400 ~/.ssh/nome-da-chave.pem

Utilize o comando abaixo substituindo pelo nome da sua chave `.pem` e o IP pÃºblico da instÃ¢ncia:

```bash
ssh -i "nome-da-chave.pem" ubuntu@<IP-Publico-da-EC2>

## âš™ï¸ 6. PreparaÃ§Ã£o dentro da EC2

ApÃ³s conectar na EC2:

1. Clonar novamente o repositÃ³rio:

```bash
git clone https://github.com/MatheusNBrito/etl-pipeline-aws-real-case.git
cd etl-pipeline-aws-real-case

2. Subir os contÃªineres:

```bash
docker compose up -d --build

## ğŸŒ 7. Acessar o Airflow

No navegador, acesse: http://<IP-Publico-da-EC2>:8081

Login padrÃ£o:

- **UsuÃ¡rio**: `admin`
- **Senha**: `airflow`

## ğŸ“Š 8. Executar as DAGs

- Ativar e rodar manualmente as DAGs `clientes_pipeline` e `vendas_pipeline` pela interface do Airflow.

## âœ… 9. ValidaÃ§Ã£o no S3

Verifique no bucket configurado se as camadas foram criadas corretamente:

- `processed/clientes/`
- `processed/vendas/`
- `gold/clientes/`
- `gold/vendas/`

Os arquivos `.parquet` estarÃ£o armazenados nas respectivas pastas.

## ğŸ›‘ 10. FinalizaÃ§Ã£o

Para encerrar o ambiente e parar os contÃªineres:

```bash
docker compose down

### ğŸ’£ Destruir toda a infraestrutura

Caso deseje remover todos os recursos provisionados na AWS:

```bash
docker run -it --rm \
  -v $(pwd):/app \
  -v ~/.aws:/root/.aws \
  -w /app \
  -e AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id) \
  -e AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) \
  hashicorp/terraform:light destroy -var-file=terraform.tfvars


## ğŸ“¥ 0. Download dos Dados Brutos

Os arquivos `.parquet` e `.json` utilizados por este pipeline sÃ£o especÃ­ficos e necessÃ¡rios para a execuÃ§Ã£o correta do projeto. Por questÃµes de tamanho, eles nÃ£o estÃ£o neste repositÃ³rio.


### ğŸ”— Link para download:
ğŸ“ [Clique aqui para acessar os arquivos brutos no Google Drive](https://drive.google.com/drive/folders/1ugcCETCJ2-zcX6oGHHbKXggrYiKRXmR7?usp=drive_link)


ğŸ“˜ Para detalhes completos sobre arquitetura, execuÃ§Ã£o e fluxos de dados, acesse a  
[ğŸ“„ DocumentaÃ§Ã£o TÃ©cnica Completa](https://pointed-growth-de1.notion.site/Documenta-o-T-cnica-Pipeline-de-Engenharia-de-Dados-123325ce8372807abd80ff81df657dfb?pvs=73)


---

---

ğŸ‘¨â€ğŸ’» Projeto desenvolvido por [**Matheus Brito**](https://www.linkedin.com/in/matheusnbrito/), 
Engenheiro de Dados  

ğŸ”— [LinkedIn](https://www.linkedin.com/in/matheusnbrito/) | ğŸ”— [GitHub](https://github.com/MatheusNBrito)
