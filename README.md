# ETL Pipeline AWS Real Case

## ğŸ“Œ VisÃ£o Geral

Este projeto implementa uma pipeline ETL utilizando Apache Airflow e PySpark, orquestrando a transformaÃ§Ã£o de dados em um Datalake estruturado com as camadas Bronze, Silver e Gold, armazenado no Amazon S3. Toda a infraestrutura Ã© gerenciada via Terraform e monitorada com AWS CloudWatch.

## ğŸ› ï¸ Tecnologias

- **Apache Airflow**
- **PySpark**
- **Terraform**
- **AWS (S3, EC2, CloudWatch)**
- **Docker**

## ğŸš§ Estrutura do Projeto

```
.
â”œâ”€â”€ dags/                   # DAGs do Airflow
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/
â”‚       â””â”€â”€ datapipelines/  # CÃ³digo dos pipelines (Bronze, Silver, Gold)
â”œâ”€â”€ IaC/                    # CÃ³digo para provisionar recursos AWS (Terraform)
â””â”€â”€ dockerfile              # Ambiente local com Docker
```

## ğŸš€ PrÃ³ximos passos

- [ ] Implementar o Airflow
- [ ] Subir para a AWS
- [ ] Integrar CI/CD.



